use tantivy::schema::{
    self, Field, IndexRecordOption, Schema, SchemaBuilder, TextFieldIndexing, TextOptions,
};
use tantivy::tokenizer::{Token, Tokenizer, TokenStream};
use tantivy::{
    schema::{BytesOptions, INDEXED, FAST},
    DateTime, Index, TantivyDocument, Term,
};
use tantivy::query::{BooleanQuery, Occur, Query, TermQuery};
use tantivy::tokenizer::NgramTokenizer;

pub mod fixed_json_layer {
    use super::*;
    use serde_json::{Map, Value};
    use std::path::Path;
    use tantivy::query::{BooleanQuery, Occur, Query, TermQuery};

    /// è‡ªå®šä¹‰è·¯å¾„å‰ç¼€åˆ†è¯å™¨ - å®ç° Tantivy Tokenizer trait
    /// è¾“å…¥æ ¼å¼ï¼špath__separator__actual_text
    /// è¾“å‡ºï¼šå¯¹actual_textåˆ†è¯ï¼Œæ¯ä¸ªtokenåŠ ä¸Špath__separator__å‰ç¼€
    #[derive(Clone)]
    pub struct PathPrefixTokenizer {
        path_separator: String,
    }

    impl PathPrefixTokenizer {
        pub fn new(path_separator: String) -> Self {
            Self { path_separator }
        }

        /// è¾…åŠ©æ–¹æ³•ï¼šæ‰‹åŠ¨åˆ†è¯å¹¶è¿”å›tokenå­—ç¬¦ä¸²åˆ—è¡¨
        pub fn tokenize_to_strings(&mut self, text: &str) -> Vec<String> {
            let mut token_stream = self.token_stream(text);
            let mut tokens = Vec::new();

            while token_stream.advance() {
                tokens.push(token_stream.token().text.clone());
            }

            tokens
        }
    }

    impl Tokenizer for PathPrefixTokenizer {
        type TokenStream<'a> = PathPrefixTokenStream;

        fn token_stream<'a>(&mut self, text: &'a str) -> Self::TokenStream<'a> {
            PathPrefixTokenStream::new(text, &self.path_separator)
        }
    }

    /// è·¯å¾„å‰ç¼€Tokenæµ - å®ç° TokenStream trait
    pub struct PathPrefixTokenStream {
        tokens: Vec<Token>,
        current_index: usize,
    }

    impl PathPrefixTokenStream {
        fn new(text: &str, path_separator: &str) -> Self {
            let mut tokens = Vec::new();

            // æŸ¥æ‰¾æœ€åä¸€ä¸ªè·¯å¾„åˆ†éš”ç¬¦çš„ä½ç½®
            if let Some(last_sep_pos) = text.rfind(path_separator) {
                let path_prefix = &text[..last_sep_pos + path_separator.len()];
                let actual_text = &text[last_sep_pos + path_separator.len()..];

                // ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†å‰²
                let words: Vec<&str> = actual_text
                    .split_whitespace()
                    .flat_map(|word| word.split(|c: char| !c.is_alphanumeric()))
                    .filter(|token| !token.is_empty() && token.len() > 2)
                    .collect();

                if words.is_empty() {
                    // å¦‚æœåˆ†è¯ç»“æœä¸ºç©ºï¼ˆæ¯”å¦‚åŸå§‹æ–‡æœ¬å°±æ˜¯"__"ï¼‰ï¼Œåˆ™å°†åŸå§‹æ–‡æœ¬ä½œä¸ºä¸€ä¸ªtoken
                    tokens.push(Token {
                        offset_from: 0,
                        offset_to: text.len(),
                        position: 0,
                        text: text.to_string(),
                        position_length: 1,
                    });
                } else {
                    for (position, token) in words.iter().enumerate() {
                        let prefixed_token = format!("{}{}", path_prefix, token.to_lowercase());
                        tokens.push(Token {
                            offset_from: 0,
                            offset_to: prefixed_token.len(),
                            position,
                            text: prefixed_token,
                            position_length: 1,
                        });
                    }
                }
            } else {
                // å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªtoken
                tokens.push(Token {
                    offset_from: 0,
                    offset_to: text.len(),
                    position: 0,
                    text: text.to_string(),
                    position_length: 1,
                });
            }

            Self {
                tokens,
                current_index: 0,
            }
        }
    }

    impl TokenStream for PathPrefixTokenStream {
        fn advance(&mut self) -> bool {
            if self.current_index < self.tokens.len() {
                self.current_index += 1;
                true
            } else {
                false
            }
        }

        fn token(&self) -> &Token {
            &self.tokens[self.current_index - 1]
        }

        fn token_mut(&mut self) -> &mut Token {
            &mut self.tokens[self.current_index - 1]
        }
    }

/// N-gram åˆ†è¯å™¨ï¼Œä¿ç•™è·¯å¾„å‰ç¼€
#[derive(Clone)]
pub struct PathPrefixNgramTokenizer {
    path_separator: String,
    ngram_tokenizer: NgramTokenizer,
}

impl PathPrefixNgramTokenizer {
    pub fn new(path_separator: String, min_gram: usize, max_gram: usize) -> Self {
        Self {
            path_separator,
            ngram_tokenizer: NgramTokenizer::new(min_gram, max_gram, false).unwrap(),
        }
    }
}

impl Tokenizer for PathPrefixNgramTokenizer {
    type TokenStream<'a> = PathPrefixNgramTokenStream;

    fn token_stream<'a>(&mut self, text: &'a str) -> Self::TokenStream<'a> {
        PathPrefixNgramTokenStream::new(
            text,
            &self.path_separator,
            self.ngram_tokenizer.clone(),
        )
    }
}

/// PathPrefixNgramTokenizer çš„ TokenStream
pub struct PathPrefixNgramTokenStream {
    tokens: Vec<Token>,
    current_index: usize,
}

impl PathPrefixNgramTokenStream {
    fn new(text: &str, path_separator: &str, mut ngram_tokenizer: NgramTokenizer) -> Self {
        let mut tokens = Vec::new();
        let mut position = 0;

        if let Some(last_sep_pos) = text.rfind(path_separator) {
            let path_prefix = &text[..last_sep_pos + path_separator.len()];
            let actual_text = &text[last_sep_pos + path_separator.len()..];

            let mut ngram_token_stream = ngram_tokenizer.token_stream(actual_text);
            while ngram_token_stream.advance() {
                let ngram_token = ngram_token_stream.token();
                let prefixed_token_text = format!("{}{}", path_prefix, ngram_token.text);
                tokens.push(Token {
                    offset_from: 0,
                    offset_to: prefixed_token_text.len(),
                    position,
                    text: prefixed_token_text,
                    position_length: 1,
                });
                position += 1;
            }
        } else {
            // No separator, just ngram the whole thing (fallback)
            let mut ngram_token_stream = ngram_tokenizer.token_stream(text);
            while ngram_token_stream.advance() {
                let ngram_token = ngram_token_stream.token().clone(); // clone to avoid borrow issues
                tokens.push(Token {
                    offset_from: ngram_token.offset_from,
                    offset_to: ngram_token.offset_to,
                    position,
                    text: ngram_token.text,
                    position_length: 1,
                });
                position += 1;
            }
        }

        Self {
            tokens,
            current_index: 0,
        }
    }
}

impl TokenStream for PathPrefixNgramTokenStream {
    fn advance(&mut self) -> bool {
        if self.current_index < self.tokens.len() {
            self.current_index += 1;
            true
        } else {
            false
        }
    }

    fn token(&self) -> &Token {
        &self.tokens[self.current_index - 1]
    }

    fn token_mut(&mut self) -> &mut Token {
        &mut self.tokens[self.current_index - 1]
    }
}

mod value_coder {
    use tantivy::DateTime;

    pub fn encode_f64(val: f64) -> [u8; 8] {
        // å°† f64 ç¼–ç ä¸ºä¿æŒæ’åºæ€§çš„ u64
        // æ­£æ•°: sign bit è®¾ä¸º 1
        // è´Ÿæ•°: æ‰€æœ‰ bit ä½å–å
        let u64_val = val.to_bits();
        let sortable_u64 = if val >= 0.0 {
            u64_val | (1u64 << 63)
        } else {
            !u64_val
        };
        sortable_u64.to_be_bytes()
    }

    pub fn encode_date(val: DateTime) -> [u8; 8] {
        // å°† i64 ç¼–ç ä¸ºä¿æŒæ’åºæ€§çš„ u64 (Sign-flipping)
        let i64_val = val.into_timestamp_micros();
        let sortable_u64 = (i64_val as u64) ^ (1u64 << 63);
        sortable_u64.to_be_bytes()
    }
}

/// ä¼˜åŒ–ç‰ˆ JSON å¤„ç†å±‚ - æ‰å¹³ç»“æ„ + è‡ªå®šä¹‰åˆ†è¯å™¨ + ç£ç›˜æŒä¹…åŒ–
#[derive(Clone)]
pub struct FixedJsonLayer {
    text_analyzed_field: Field, // åˆ†è¯æ–‡æœ¬å­—æ®µï¼ˆä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼‰
    text_raw_field: Field,      // åŸå§‹æ–‡æœ¬å­—æ®µï¼ˆrawåˆ†è¯å™¨ï¼‰
    text_ngram_field: Field,    // N-gram å­—æ®µï¼Œç”¨äºéƒ¨åˆ†åŒ¹é…
    number_field: Field,        // æ•°å€¼å­—æ®µ
    date_field: Field,          // æ—¥æœŸå­—æ®µ
    schema: Schema,
    config: JsonLayerConfig,
    path_tokenizer: PathPrefixTokenizer, // è‡ªå®šä¹‰è·¯å¾„å‰ç¼€åˆ†è¯å™¨
}

/// é…ç½®
#[derive(Clone)]
pub struct JsonLayerConfig {
    pub path_separator: String,
    pub text_classification_rules: TextClassificationRules,
}

/// ç®€åŒ–ç‰ˆæ–‡æœ¬åˆ†ç±»è§„åˆ™
#[derive(Clone)]
pub struct TextClassificationRules {
    pub identifier_patterns: Vec<regex::Regex>,
}

impl Default for JsonLayerConfig {
    fn default() -> Self {
        Self {
            path_separator: "__".to_string(),
            text_classification_rules: TextClassificationRules::default(),
        }
    }
}

impl Default for TextClassificationRules {
    fn default() -> Self {
        Self {
            identifier_patterns: vec![
                regex::Regex::new(r"^[A-Z0-9]{6,}$").unwrap(), // å¤§å†™ID
                regex::Regex::new(r"^[a-z0-9]+@[a-z0-9]+\.[a-z]+$").unwrap(), // é‚®ç®±åœ°å€
                regex::Regex::new(r"^[A-Z]{2,3}[0-9]{6,}$").unwrap(), // äº§å“SKUæ ¼å¼
            ],
        }
    }
}

/// æ–‡æœ¬ç±»å‹åˆ†ç±»
#[derive(Debug, Clone)]
enum TextType {
    AnalyzedText, // éœ€è¦åˆ†è¯çš„æ–‡æœ¬
    Keyword,      // çŸ­å…³é”®è¯
    Identifier,   // æ ‡è¯†ç¬¦
}

impl FixedJsonLayer {
    pub fn new() -> tantivy::Result<Self> {
        Self::new_with_config(JsonLayerConfig::default())
    }

    pub fn new_with_config(config: JsonLayerConfig) -> tantivy::Result<Self> {
        let mut schema_builder = SchemaBuilder::new();

        // ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨åç§°
        let text_analyzed_field = schema_builder.add_text_field(
            "json_text_analyzed",
            TextOptions::default()
                .set_indexing_options(
                    TextFieldIndexing::default()
                        .set_tokenizer("path_prefix") // ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼
                        .set_index_option(IndexRecordOption::Basic),
                )
                .set_stored(),
        );

        let text_raw_field = schema_builder.add_text_field(
            "json_text_raw",
            TextOptions::default()
                .set_indexing_options(
                    TextFieldIndexing::default()
                        .set_tokenizer("raw")
                        .set_index_option(IndexRecordOption::Basic),
                )
                .set_stored(),
        );

        // N-gram å­—æ®µ
        let text_ngram_field = schema_builder.add_text_field(
            "json_text_ngram",
            TextOptions::default().set_indexing_options(
                TextFieldIndexing::default()
                    .set_tokenizer("path_prefix_ngram")
                    .set_index_option(IndexRecordOption::Basic),
            ),
        );

        // å°† number_field å’Œ date_field å®šä¹‰ä¸º text å­—æ®µï¼Œä½¿ç”¨ raw åˆ†è¯å™¨
        // ä»¥ä¾¿å­˜å‚¨ `path + encoded_value` å¹¶æ”¯æŒèŒƒå›´æŸ¥è¯¢
        let number_field = schema_builder.add_bytes_field(
            "json_number",
            BytesOptions::default().set_indexed().set_fast(),
        );
        let date_field = schema_builder.add_bytes_field(
            "json_date",
            BytesOptions::default().set_indexed().set_fast(),
        );

        let schema = schema_builder.build();

        let path_tokenizer = PathPrefixTokenizer::new(config.path_separator.clone());

        Ok(FixedJsonLayer {
            text_analyzed_field,
            text_raw_field,
            text_ngram_field,
            number_field,
            date_field,
            schema,
            config,
            path_tokenizer,
        })
    }

    /// åˆ›å»ºæˆ–æ‰“å¼€ç£ç›˜ç´¢å¼•
    pub fn create_or_open_index<P: AsRef<Path>>(
        &self,
        index_path: P,
    ) -> tantivy::Result<Index> {
        let index_path = index_path.as_ref();

        let index = if index_path.exists() {
            // æ‰“å¼€ç°æœ‰ç´¢å¼•
            println!("ğŸ“‚ Opening existing index at: {:?}", index_path);
            Index::open_in_dir(index_path)?
        } else {
            // åˆ›å»ºæ–°ç´¢å¼•
            println!("ğŸ†• Creating new index at: {:?}", index_path);
            std::fs::create_dir_all(index_path)?;
            Index::create_in_dir(index_path, self.schema.clone())?
        };

        // æ³¨å†Œè‡ªå®šä¹‰åˆ†è¯å™¨
        let tokenizers = index.tokenizers();
        tokenizers.register(
            "path_prefix",
            PathPrefixTokenizer::new(self.config.path_separator.clone()),
        );
        // æ³¨å†Œ n-gram åˆ†è¯å™¨ (min=2, max=3)
        tokenizers.register(
            "path_prefix_ngram",
            PathPrefixNgramTokenizer::new(self.config.path_separator.clone(), 2, 3),
        );

        Ok(index)
    }

    pub fn schema(&self) -> &Schema {
        &self.schema
    }

    /// å¤„ç†æ‰å¹³ JSON å¯¹è±¡ï¼ˆä¸æ”¯æŒåµŒå¥—ï¼‰
    pub fn process_flat_json_object(
        &self,
        json_obj: &Map<String, Value>,
    ) -> tantivy::Result<TantivyDocument> {
        let mut doc = TantivyDocument::new();

        for (key, value) in json_obj {
            self.add_flat_value(&mut doc, key, value);
        }

        Ok(doc)
    }

    /// æ·»åŠ æ‰å¹³JSONå€¼ï¼ˆå¤„ç†æ•°ç»„å’ŒåŸºæœ¬ç±»å‹ï¼‰
    fn add_flat_value(&self, doc: &mut TantivyDocument, field_name: &str, value: &Value) {
        match value {
            Value::String(s) => {
                // å°è¯•è§£æä¸ºæ—¥æœŸï¼Œå¤±è´¥åˆ™ä½œä¸ºæ–‡æœ¬å¤„ç†
                if let Some(date_time) = self.try_parse_date(s) {
                    self.add_date_value(doc, field_name, date_time);
                } else {
                    let text_type = self.classify_text(s);
                    self.add_text_value(doc, field_name, s, text_type);
                }
            }
            Value::Number(n) => {
                if let Some(f) = n.as_f64() {
                    self.add_number_value(doc, field_name, f);
                }
            }
            Value::Bool(b) => {
                self.add_bool_value(doc, field_name, *b);
            }
            Value::Array(arr) => {
                // å¤„ç†æ•°ç»„ï¼šä¸ºæ¯ä¸ªå…ƒç´ æ·»åŠ ç›¸åŒçš„å­—æ®µå
                for item in arr {
                    self.add_flat_value(doc, field_name, item);
                }
            }
            _ => {
                // å¿½ç•¥ null å’Œå…¶ä»–ç±»å‹
            }
        }
    }

    /// å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
    fn try_parse_date(&self, s: &str) -> Option<DateTime> {
        if s.len() < 8 {
            return None;
        }
        // æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼çš„åŸºæœ¬ç‰¹å¾
        let has_date_chars = s.contains('-') || s.contains('T') || s.contains(':');
        if !has_date_chars {
            return None;
        }
        self.parse_date_formats(s)
    }

    /// è§£æå¤šç§æ—¥æœŸæ ¼å¼
    fn parse_date_formats(&self, s: &str) -> Option<DateTime> {
        use time::{Date, PrimitiveDateTime, Time};
        if let Ok(dt) =
            time::OffsetDateTime::parse(s, &time::format_description::well_known::Iso8601::DEFAULT)
        {
            return Some(DateTime::from_utc(dt));
        }
        if let Ok(dt) =
            PrimitiveDateTime::parse(s, &time::format_description::well_known::Iso8601::DEFAULT)
        {
            let offset_dt = dt.assume_utc();
            return Some(DateTime::from_utc(offset_dt));
        }
        if s.len() == 10 && s.matches('-').count() == 2 {
            if let Ok(date) =
                Date::parse(s, &time::format_description::parse("[year]-[month]-[day]").unwrap())
            {
                let dt = PrimitiveDateTime::new(date, Time::MIDNIGHT);
                return Some(DateTime::from_utc(dt.assume_utc()));
            }
        }
        None
    }

    /// æ·»åŠ æ—¥æœŸå€¼
    fn add_date_value(&self, doc: &mut TantivyDocument, field_name: &str, date_time: DateTime) {
        let encoded_date = value_coder::encode_date(date_time);
        let mut path_value =
            format!("{}{}", field_name, self.config.path_separator).into_bytes();
        path_value.extend_from_slice(&encoded_date);
        doc.add_bytes(self.date_field, &path_value);
    }

    /// ç®€åŒ–çš„æ–‡æœ¬åˆ†ç±»
    fn classify_text(&self, text: &str) -> TextType {
        for pattern in &self.config.text_classification_rules.identifier_patterns {
            if pattern.is_match(text) {
                return TextType::Identifier;
            }
        }
        if self.has_whitespace_or_punctuation(text) {
            TextType::AnalyzedText
        } else {
            TextType::Keyword
        }
    }

    /// æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·
    fn has_whitespace_or_punctuation(&self, text: &str) -> bool {
        text.chars()
            .any(|c| c.is_whitespace() || c.is_ascii_punctuation() || !c.is_alphanumeric())
    }

    /// æ·»åŠ æ–‡æœ¬å€¼ - æ™ºèƒ½åˆ†è¯ç­–ç•¥
    fn add_text_value(
        &self,
        doc: &mut TantivyDocument,
        path: &str,
        value: &str,
        text_type: TextType,
    ) {
        let prefixed_value = format!("{}{}{}", path, self.config.path_separator, value);

        match text_type {
            TextType::AnalyzedText => {
                doc.add_text(self.text_raw_field, &prefixed_value);
                let tokens = self
                    .path_tokenizer
                    .clone()
                    .tokenize_to_strings(&prefixed_value);
                for token in tokens {
                    doc.add_text(self.text_analyzed_field, &token);
                }
                // ä¸ºå¯åˆ†ææ–‡æœ¬æ·»åŠ å¸¦è·¯å¾„çš„ n-gram ç´¢å¼•
                doc.add_text(self.text_ngram_field, &prefixed_value);
            }
            TextType::Keyword | TextType::Identifier => {
                doc.add_text(self.text_raw_field, &prefixed_value);
            }
        }
    }

    /// æ·»åŠ æ•°å€¼
    fn add_number_value(&self, doc: &mut TantivyDocument, path: &str, value: f64) {
        let encoded_num = value_coder::encode_f64(value);
        let mut path_value = format!("{}{}", path, self.config.path_separator).into_bytes();
        path_value.extend_from_slice(&encoded_num);
        doc.add_bytes(self.number_field, &path_value);
    }

    /// æ·»åŠ å¸ƒå°”å€¼
    fn add_bool_value(&self, doc: &mut TantivyDocument, path: &str, value: bool) {
        let path_value = format!("{}{}{}", path, self.config.path_separator, value);
        doc.add_text(self.text_raw_field, &path_value);
    }
}

/// æ™ºèƒ½æŸ¥è¯¢æ„å»ºå™¨
pub struct SmartJsonQueryBuilder {
    layer: FixedJsonLayer,
}

impl SmartJsonQueryBuilder {
    pub fn new(layer: FixedJsonLayer) -> Self {
        Self { layer }
    }

    /// æ™ºèƒ½æŸ¥è¯¢: å¯¹æŸ¥è¯¢è¯åˆ†è¯ï¼Œå¹¶åŒæ—¶æœç´¢åŸæ–‡å’Œè¯å…ƒ
    pub fn smart_query(
        &self,
        path: &str,
        value: &str,
    ) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        let mut subqueries: Vec<(Occur, Box<dyn Query>)> = Vec::new();

        // 1. åŸå§‹å­—æ®µæŸ¥è¯¢ï¼ˆç²¾ç¡®åŒ¹é…æ•´ä¸ªæŸ¥è¯¢å­—ç¬¦ä¸²ï¼‰
        let prefixed_value = format!("{}{}{}", path, self.layer.config.path_separator, value);
        let raw_term = Term::from_field_text(self.layer.text_raw_field, &prefixed_value);
        subqueries.push((
            Occur::Should,
            Box::new(TermQuery::new(raw_term, IndexRecordOption::Basic)),
        ));

        // 2. åˆ†è¯å­—æ®µæŸ¥è¯¢ (ANDæŸ¥è¯¢ï¼Œè¦æ±‚æ‰€æœ‰è¯å…ƒéƒ½å­˜åœ¨)
        let mut tokenizer = self.layer.path_tokenizer.clone();
        let tokens = tokenizer.tokenize_to_strings(&prefixed_value);

        // åªæœ‰å½“åˆ†è¯ç»“æœå¤šäºä¸€ä¸ªï¼Œæˆ–è€…å•ä¸ªåˆ†è¯ä¸åŸå§‹å€¼ä¸åŒæ—¶ï¼Œæ‰è¿›è¡Œåˆ†è¯æŸ¥è¯¢
        if tokens.len() > 1 || (tokens.len() == 1 && tokens[0] != prefixed_value) {
            let mut token_queries = Vec::new();
            for token_str in tokens {
                let term = Term::from_field_text(self.layer.text_analyzed_field, &token_str);
                token_queries.push((
                    Occur::Must,
                    Box::new(TermQuery::new(term, IndexRecordOption::Basic)) as Box<dyn Query>,
                ));
            }
            if !token_queries.is_empty() {
                subqueries.push((Occur::Should, Box::new(BooleanQuery::new(token_queries))));
            }
        }

        Ok(Box::new(BooleanQuery::new(subqueries)))
    }

    /// N-gram éƒ¨åˆ†åŒ¹é…æŸ¥è¯¢
    pub fn ngram_query_with_path(
        &self,
        path: &str,
        value: &str,
    ) -> tantivy::Result<Box<dyn Query>> {
        use tantivy::query::{BooleanQuery, EmptyQuery, Occur, Query, TermQuery};
        use tantivy::tokenizer::Tokenizer;

        // å¿…é¡»ä½¿ç”¨ä¸ç´¢å¼•æ—¶ç›¸åŒçš„ n-gram é…ç½®æ¥åˆ‡åˆ†æŸ¥è¯¢è¯
        let mut tokenizer = NgramTokenizer::new(2, 3, false)?;
        let mut token_stream = tokenizer.token_stream(value);
        let path_prefix = format!("{}{}", path, self.layer.config.path_separator);

        let mut subqueries = Vec::new();
        while token_stream.advance() {
            let ngram_text = &token_stream.token().text;
            let term_text = format!("{}{}", path_prefix, ngram_text);
            let term = Term::from_field_text(self.layer.text_ngram_field, &term_text);
            subqueries.push((
                Occur::Must,
                Box::new(TermQuery::new(term, IndexRecordOption::Basic)) as Box<dyn Query>,
            ));
        }

        if subqueries.is_empty() {
            return Ok(Box::new(EmptyQuery {}));
        }

        Ok(Box::new(BooleanQuery::new(subqueries)))
    }

    /// ç²¾ç¡®åŒ¹é…æŸ¥è¯¢ (åªæŸ¥rawå­—æ®µ)
    pub fn exact_query(
        &self,
        path: &str,
        value: &str,
    ) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        let prefixed_value = format!("{}{}{}", path, self.layer.config.path_separator, value);
        let term = Term::from_field_text(self.layer.text_raw_field, &prefixed_value);
        Ok(Box::new(TermQuery::new(
            term,
            IndexRecordOption::Basic,
        )))
    }

    /// å¸¦è·¯å¾„çš„æ•°å€¼èŒƒå›´æŸ¥è¯¢
    pub fn number_range_query_with_path(
        &self,
        path: &str,
        min: f64,
        max: f64,
    ) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use std::ops::Bound;
        use tantivy::query::RangeQuery;

        let path_prefix_bytes = format!("{}{}", path, self.layer.config.path_separator).into_bytes();

        let mut min_bytes = path_prefix_bytes.clone();
        min_bytes.extend_from_slice(&value_coder::encode_f64(min));

        let mut max_bytes = path_prefix_bytes;
        max_bytes.extend_from_slice(&value_coder::encode_f64(max));

        let min_term = Term::from_field_bytes(self.layer.number_field, &min_bytes);
        let max_term = Term::from_field_bytes(self.layer.number_field, &max_bytes);

        let range_query =
            RangeQuery::new(Bound::Included(min_term), Bound::Included(max_term));

        Ok(Box::new(range_query))
    }

    /// å¸¦è·¯å¾„çš„æ—¥æœŸèŒƒå›´æŸ¥è¯¢
    pub fn date_range_query_with_path(
        &self,
        path: &str,
        start_date: &str,
        end_date: &str,
    ) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use std::ops::Bound;
        use tantivy::query::RangeQuery;

        let start_dt = self.layer.parse_date_formats(start_date).ok_or_else(|| {
            tantivy::TantivyError::InvalidArgument(format!(
                "Cannot parse start date: {}",
                start_date
            ))
        })?;
        let end_dt = self.layer.parse_date_formats(end_date).ok_or_else(|| {
            tantivy::TantivyError::InvalidArgument(format!("Cannot parse end date: {}", end_date))
        })?;

        let path_prefix_bytes = format!("{}{}", path, self.layer.config.path_separator).into_bytes();

        let start_dt_bytes = value_coder::encode_date(start_dt);
        let mut min_bytes = path_prefix_bytes.clone();
        min_bytes.extend_from_slice(&start_dt_bytes);

        let end_dt_bytes = value_coder::encode_date(end_dt);
        let mut max_bytes = path_prefix_bytes;
        max_bytes.extend_from_slice(&end_dt_bytes);

        let start_term = Term::from_field_bytes(self.layer.date_field, &min_bytes);
        let end_term = Term::from_field_bytes(self.layer.date_field, &max_bytes);

        let range_query =
            RangeQuery::new(Bound::Included(start_term), Bound::Included(end_term));

        Ok(Box::new(range_query))
    }
}
}

fn main() -> tantivy::Result<()> {
    use fixed_json_layer::{FixedJsonLayer, SmartJsonQueryBuilder};
    use serde_json::json;
    use tantivy::collector::TopDocs;
    use tantivy::query::Query;
    use tantivy::Searcher;

    println!("ğŸš€ Fixed JSON Layer Example ğŸš€");

    // 1. è®¾ç½®å’Œç´¢å¼•
    let layer = FixedJsonLayer::new()?;
    let index_path = "./json_index_refined"; // ä½¿ç”¨æ–°çš„ç›®å½•ä»¥é¿å…å†²çª
    let index = layer.create_or_open_index(index_path)?;
    // ä½¿ç”¨å•çº¿ç¨‹å†™å…¥ï¼Œä»¥ä¿è¯åœ¨è¿™ä¸ªå°ä¾‹å­ä¸­æ‰€æœ‰æ–‡æ¡£éƒ½åœ¨ä¸€ä¸ªæ®µå†…ï¼Œä½¿å¾— doc_id è¿ç»­
    let mut index_writer = index.writer_with_num_threads(1, 50_000_000)?;

    // æ¸…ç†æ—§æ•°æ®ï¼ˆä»…ä¸ºç¤ºä¾‹ï¼‰
    index_writer.delete_all_documents()?;
    index_writer.commit()?;

    let test_documents = vec![
        json!({
            "user_name": "Alice Smith",
            "user_email": "alice@example.com",
            "user_age": 28,
            "user_tags": ["rust", "search", "database"],
            "product_active": true,
            "product_description": "A high-quality search engine library for Rust applications",
            "user_created_at": "2024-01-15T10:30:00Z",
        }),
        json!({
            "company_name": "Tech Innovations Inc",
            "company_country": "USA",
            "company_city": "San Francisco",
            "company_coordinates": [37.7749, -122.4194],
            "company_established_date": "2020-01-15T09:00:00Z",
            "user_age": 27
        }),
        
        // æ–‡æ¡£3ï¼šç”µå•†äº§å“ï¼ˆæ‰å¹³ç»“æ„ï¼‰
        json!({
            "product_name": "Wireless Headphones",
            "product_sku": "WH001234",
            "product_price": 149.99,
            "review_ratings": [5, 4, 5, 3, 4],
            "review_comments": ["excellent quality", "great sound", "comfortable","library"],
            "review_verified": [true, true, false, true, true],
            "inventory_stock": 50,
            "inventory_colors": ["black", "white", "blue"],
            "company_established_date": "2020-11-15T09:00:00Z",
            "inventory_availability": true,
            "product_launch_date": "2024-02-14",
            "test_wrong":25,
            "user_age": 80,
            "inventory_last_updated": "2024-07-21T08:30:00Z"
        }),
        
        // æ–‡æ¡£4ï¼šå­¦æœ¯è®ºæ–‡ï¼ˆæ‰å¹³ç»“æ„ï¼‰
        json!({
            "paper_title": "Advanced Information Retrieval Systems",
            "paper_authors": ["Dr. John Smith", "Prof. Jane Doe"],
            "paper_year": 2023,
            "test_wrong":149.99,
            "product_price": 19.99,
            "metrics_citations": 42,
            "metrics_downloads": [120, 95, 87, 76],
            "metrics_impact_factor": 2.8,
            "research_keywords": ["information retrieval", "search engines", "natural language processing"],
            "paper_published_date": "2023-05-20T12:00:00Z",
            "company_established_date": "2020-08-15",
            "paper_submitted_date": "2023-02-28",
            "metrics_last_calculated": "2024-07-15T10:00:00Z"
        })
    ];

    for (i, json_data) in test_documents.iter().enumerate() {
        if let serde_json::Value::Object(obj) = json_data {
            let doc = layer.process_flat_json_object(obj)?;
            index_writer.add_document(doc)?;
            println!("âœ… Document {} indexed.", i + 1);
        }
    }
    index_writer.commit()?;
    let reader = index.reader()?;
    let searcher = reader.searcher();
    let query_builder = SmartJsonQueryBuilder::new(layer.clone());

    // 2. è¿è¡Œæ ¸å¿ƒæŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ” Running Core Query Tests...");

    // è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ‰§è¡ŒæŸ¥è¯¢å¹¶æ‰“å°ç»“æœ
    fn run_query_and_print_results(
        searcher: &Searcher,
        query: Box<dyn Query>,
        description: &str,
    ) -> tantivy::Result<()> {
        let top_docs = searcher.search(&*query, &TopDocs::with_limit(5))?;
        println!("\n---");
        println!("ğŸ’¬ Query: {}", description);
        println!("ğŸ¯ Found {} documents.", top_docs.len());
        for (_score, doc_address) in top_docs {
            println!("   - Doc Address: {:?}", doc_address);
        }
        Ok(())
    }

    // --- æµ‹è¯•ç”¨ä¾‹ ---

    // a. å…³é”®è¯æŸ¥è¯¢ (ç²¾ç¡®åŒ¹é…)
    let query = query_builder.smart_query("company_country", "USA")?;
    run_query_and_print_results(&searcher, query, "Keyword search for country 'USA'")?;

    // b. åˆ†è¯æŸ¥è¯¢
    let query = query_builder.smart_query("product_description", "search library")?;
    run_query_and_print_results(
        &searcher,
        query,
        "Tokenized search for 'search library' in description",
    )?;

    // c. æ•°ç»„ä¸­çš„ç²¾ç¡®åŒ¹é…
    let query = query_builder.smart_query("user_tags", "rust")?;
    run_query_and_print_results(&searcher, query, "Exact match for 'rust' in tags array")?;

    // d. æ•°å€¼èŒƒå›´æŸ¥è¯¢
    let query = query_builder.number_range_query_with_path("user_age", 25.0, 30.0)?;
    run_query_and_print_results(&searcher, query, "Number range for age between 25 and 30")?;

    // e. æ—¥æœŸèŒƒå›´æŸ¥è¯¢
    let query = query_builder
        .date_range_query_with_path("company_established_date", "2020-01-01", "2020-12-31")?;
    run_query_and_print_results(
        &searcher,
        query,
        "Date range for establishment in year 2020",
    )?;

    // f. N-gram éƒ¨åˆ†è¯æŸ¥è¯¢
    let query = query_builder.ngram_query_with_path("product_description", "librar")?;
    run_query_and_print_results(
        &searcher,
        query,
        "N-gram search for partial word 'librar' in 'product_description'",
    )?;

    // g. æ•°ç»„æ•°å€¼èŒƒå›´æŸ¥è¯¢
    let query = query_builder.number_range_query_with_path("metrics_downloads", 80.0, 90.0)?;
    run_query_and_print_results(&searcher, query, "Number range for metrics_downloads between 80 and 90")?;

    println!("\n---\nğŸ’¡ Index Location: '{}'", index_path);

    Ok(())
} 