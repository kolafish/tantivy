use tantivy::schema::{Schema, SchemaBuilder, TextFieldIndexing, TextOptions, NumericOptions, Field, IndexRecordOption};
use tantivy::{Index, TantivyDocument, Term, DateTime};
use tantivy::tokenizer::{Tokenizer, TokenStream, Token};
use serde_json::{Value, Map};
use std::path::Path;

/// è‡ªå®šä¹‰è·¯å¾„å‰ç¼€åˆ†è¯å™¨ - å®ç° Tantivy Tokenizer trait
/// è¾“å…¥æ ¼å¼ï¼špath__separator__actual_text
/// è¾“å‡ºï¼šå¯¹actual_textåˆ†è¯ï¼Œæ¯ä¸ªtokenåŠ ä¸Špath__separator__å‰ç¼€
#[derive(Clone)]
pub struct PathPrefixTokenizer {
    path_separator: String,
}

impl PathPrefixTokenizer {
    pub fn new(path_separator: String) -> Self {
        Self { path_separator }
    }
    
    /// è¾…åŠ©æ–¹æ³•ï¼šæ‰‹åŠ¨åˆ†è¯å¹¶è¿”å›tokenå­—ç¬¦ä¸²åˆ—è¡¨
    pub fn tokenize_to_strings(&mut self, text: &str) -> Vec<String> {
        let mut token_stream = self.token_stream(text);
        let mut tokens = Vec::new();
        
        while token_stream.advance() {
            tokens.push(token_stream.token().text.clone());
        }
        
        tokens
    }
}

impl Tokenizer for PathPrefixTokenizer {
    type TokenStream<'a> = PathPrefixTokenStream;

    fn token_stream<'a>(&mut self, text: &'a str) -> Self::TokenStream<'a> {
        PathPrefixTokenStream::new(text, &self.path_separator)
    }
}

/// è·¯å¾„å‰ç¼€Tokenæµ - å®ç° TokenStream trait
pub struct PathPrefixTokenStream {
    tokens: Vec<Token>,
    current_index: usize,
}

impl PathPrefixTokenStream {
    fn new(text: &str, path_separator: &str) -> Self {
        let mut tokens = Vec::new();
        
        // æŸ¥æ‰¾æœ€åä¸€ä¸ªè·¯å¾„åˆ†éš”ç¬¦çš„ä½ç½®
        if let Some(last_sep_pos) = text.rfind(path_separator) {
            let path_prefix = &text[..last_sep_pos + path_separator.len()];
            let actual_text = &text[last_sep_pos + path_separator.len()..];
            
            // ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†å‰²
            let words: Vec<&str> = actual_text
                .split_whitespace()
                .flat_map(|word| word.split(|c: char| !c.is_alphanumeric()))
                .filter(|token| !token.is_empty() && token.len() > 2)
                .collect();
            
            for (position, token) in words.iter().enumerate() {
                let prefixed_token = format!("{}{}", path_prefix, token.to_lowercase());
                tokens.push(Token {
                    offset_from: 0,
                    offset_to: prefixed_token.len(),
                    position: position,
                    text: prefixed_token,
                    position_length: 1,
                });
            }
        } else {
            // å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªtoken
            tokens.push(Token {
                offset_from: 0,
                offset_to: text.len(),
                position: 0,
                text: text.to_string(),
                position_length: 1,
            });
        }
        
        Self {
            tokens,
            current_index: 0,
        }
    }
}

impl TokenStream for PathPrefixTokenStream {
    fn advance(&mut self) -> bool {
        if self.current_index < self.tokens.len() {
            self.current_index += 1;
            true
        } else {
            false
        }
    }

    fn token(&self) -> &Token {
        &self.tokens[self.current_index - 1]
    }

    fn token_mut(&mut self) -> &mut Token {
        &mut self.tokens[self.current_index - 1]
    }
}

/// å€¼ç¼–ç æ¨¡å— - ç”¨äºå°†æ•°å€¼/æ—¥æœŸç­‰ç±»å‹ç¼–ç ä¸ºå¯æŒ‰å­—å…¸åºæ’åºçš„å­—ç¬¦ä¸²
///
/// ## è®¾è®¡åŠ¨æœº
///
/// åœ¨ `tantivy` ä¸­ï¼Œä¸“ç”¨çš„æ•°å€¼å­—æ®µï¼ˆå¦‚ `f64`, `date`ï¼‰é€šè¿‡ `Term` å†…éƒ¨çš„ `set_fast_value`
/// é€»è¾‘ï¼Œå°†æ•°å€¼è½¬æ¢ä¸ºä¿ç•™å…¶å¤§å°é¡ºåºçš„å¤§ç«¯å­—èŠ‚åºï¼ˆ`&[u8]`ï¼‰è¿›è¡Œå­˜å‚¨å’ŒèŒƒå›´æŸ¥è¯¢ã€‚
/// è¿™ç§äºŒè¿›åˆ¶è¡¨ç¤ºæ˜¯æœ€é«˜æ•ˆçš„ï¼Œä½†ä¸ä¸€å®šæ˜¯åˆæ³•çš„ UTF-8 å­—ç¬¦ä¸²ã€‚
///
/// åœ¨æœ¬å®ç°ä¸­ï¼Œä¸ºäº†åœ¨ä¸€ä¸ªå­—æ®µå†…åŒæ—¶å®ç° â€œè·¯å¾„è¿‡æ»¤â€ å’Œ â€œèŒƒå›´æŸ¥è¯¢â€ï¼Œæˆ‘ä»¬å°†æ•°å€¼/æ—¥æœŸå­—æ®µ
/// å®šä¹‰ä¸ºäº† `text` ç±»å‹ï¼Œå¹¶ä½¿ç”¨ `raw` åˆ†è¯å™¨ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¿…é¡»å°† `è·¯å¾„` å’Œ `å€¼`
/// æ‹¼æ¥æˆä¸€ä¸ª**å•ä¸€çš„ã€åˆæ³•çš„å­—ç¬¦ä¸²**æ¥ä½œä¸º `Term`ã€‚
///
/// ## å®ç°ç­–ç•¥
///
/// `value_coder` çš„ä½œç”¨å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜ï¼š
/// 1. **å€Ÿé‰´æ ¸å¿ƒæ€æƒ³**: é‡‡ç”¨ä¸ `tantivy` å†…éƒ¨ç›¸åŒçš„ä½æ“ä½œé€»è¾‘ï¼Œå°† `f64`/`i64` (æ¥è‡ª`DateTime`)
///    è½¬æ¢ä¸ºä¸€ä¸ªä¿ç•™åŸå§‹å¤§å°é¡ºåºçš„ `u64`ã€‚
/// 2. **é€‚é…ä¸ºå­—ç¬¦ä¸²**: å°†è¿™ä¸ª `u64` å€¼ç¼–ç ä¸ºä¸€ä¸ªå®šé•¿çš„**åå…­è¿›åˆ¶å­—ç¬¦ä¸²**ã€‚åå…­è¿›åˆ¶è¡¨ç¤ºæ³•
///    æ—¢èƒ½å®Œæ•´åœ°ä»£è¡¨åº•å±‚çš„äºŒè¿›åˆ¶æ•°æ®ï¼Œå…¶å­—å…¸åºä¹Ÿç­‰åŒäºåŸå§‹æ•°å€¼çš„é¡ºåºï¼ŒåŒæ—¶å®ƒæœ¬èº«æ˜¯
///    åˆæ³•çš„ UTF-8 å­—ç¬¦ã€‚
///
/// æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°æ„å»ºå¦‚ `product_price__800533325996fbe5` è¿™æ ·çš„å­—ç¬¦ä¸²ï¼Œ
/// å®ƒå¯ä»¥åœ¨ä¸€ä¸ª `text` å­—æ®µä¸Šé€šè¿‡ `RangeQuery` è¿›è¡Œé«˜æ•ˆçš„ã€å¸¦è·¯å¾„çš„èŒƒå›´æŸ¥æ‰¾ã€‚
mod value_coder {
    use tantivy::DateTime;

    /// å°† i64 ç¼–ç ä¸ºä¿æŒæ’åºæ€§çš„ u64 (Sign-flipping)
    fn i64_to_sortable_u64(val: i64) -> u64 {
        (val as u64) ^ (1u64 << 63)
    }

    /// å°† f64 ç¼–ç ä¸ºä¿æŒæ’åºæ€§çš„ u64
    /// æ­£æ•°: sign bit è®¾ä¸º 1
    /// è´Ÿæ•°: æ‰€æœ‰ bit ä½å–å
    fn f64_to_sortable_u64(val: f64) -> u64 {
        let u64_val = val.to_bits();
        if val >= 0.0 {
            u64_val | (1u64 << 63)
        } else {
            !u64_val
        }
    }

    pub fn encode_f64(val: f64) -> String {
        format!("{:016x}", f64_to_sortable_u64(val))
    }

    pub fn encode_date(val: DateTime) -> String {
        let i64_val = val.into_timestamp_micros();
        format!("{:016x}", i64_to_sortable_u64(i64_val))
    }
}

/// ä¼˜åŒ–ç‰ˆ JSON å¤„ç†å±‚ - æ‰å¹³ç»“æ„ + è‡ªå®šä¹‰åˆ†è¯å™¨ + ç£ç›˜æŒä¹…åŒ–
pub struct FixedJsonLayer {
    text_analyzed_field: Field,      // åˆ†è¯æ–‡æœ¬å­—æ®µï¼ˆä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼‰
    text_raw_field: Field,           // åŸå§‹æ–‡æœ¬å­—æ®µï¼ˆrawåˆ†è¯å™¨ï¼‰
    number_field: Field,             // æ•°å€¼å­—æ®µ
    bool_field: Field,               // å¸ƒå°”å­—æ®µ
    date_field: Field,               // æ—¥æœŸå­—æ®µï¼ˆé‡è¦ï¼ï¼‰
    
    schema: Schema,
    config: JsonLayerConfig,
    path_tokenizer: PathPrefixTokenizer,  // è‡ªå®šä¹‰è·¯å¾„å‰ç¼€åˆ†è¯å™¨
}

/// é…ç½®
#[derive(Clone)]
pub struct JsonLayerConfig {
    pub path_separator: String,
    pub max_path_depth: usize,
    pub text_classification_rules: TextClassificationRules,
}

/// ç®€åŒ–ç‰ˆæ–‡æœ¬åˆ†ç±»è§„åˆ™
/// 1. æ ‡è¯†ç¬¦æ¨¡å¼ (identifier_patterns)
/// - é»˜è®¤åŒ…å«: å¤§å†™IDã€é‚®ç®±åœ°å€ç­‰ç»“æ„åŒ–æ ‡è¯†ç¬¦
/// - ä½œç”¨: è¯†åˆ«ç‰¹æ®Šæ ¼å¼ï¼Œé¿å…ä¸å¿…è¦çš„åˆ†è¯
/// 2. åˆ†ç±»é€»è¾‘ç®€åŒ–:
/// - åŒ…å«ç©ºæ ¼æˆ–æ ‡ç‚¹ â†’ AnalyzedText (ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨)
/// - ç®€çŸ­æ— ç©ºæ ¼ â†’ Keyword (ä½¿ç”¨rawåˆ†è¯å™¨)
/// - åŒ¹é…æ ‡è¯†ç¬¦æ¨¡å¼ â†’ Identifier (ä½¿ç”¨rawåˆ†è¯å™¨)
#[derive(Clone)]
pub struct TextClassificationRules {
    pub identifier_patterns: Vec<regex::Regex>,
}

impl Default for JsonLayerConfig {
    fn default() -> Self {
        Self {
            path_separator: "__".to_string(),
            max_path_depth: 10,  // ä¿ç•™æ·±åº¦é™åˆ¶ï¼ˆè™½ç„¶ç°åœ¨æ˜¯æ‰å¹³ç»“æ„ï¼‰
            text_classification_rules: TextClassificationRules::default(),
        }
    }
}

impl Default for TextClassificationRules {
    fn default() -> Self {
        Self {
            identifier_patterns: vec![
                regex::Regex::new(r"^[A-Z0-9]{6,}$").unwrap(), // å¤§å†™ID
                regex::Regex::new(r"^[a-z0-9]+@[a-z0-9]+\.[a-z]+$").unwrap(), // é‚®ç®±åœ°å€
                regex::Regex::new(r"^[A-Z]{2,3}[0-9]{6,}$").unwrap(), // äº§å“SKUæ ¼å¼
            ],
        }
    }
}

impl FixedJsonLayer {
    pub fn new() -> tantivy::Result<Self> {
        Self::new_with_config(JsonLayerConfig::default())
    }
    
    pub fn new_with_config(config: JsonLayerConfig) -> tantivy::Result<Self> {
        let mut schema_builder = SchemaBuilder::new();
        
        // ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨åç§°
        let text_analyzed_field = schema_builder.add_text_field(
            "json_text_analyzed",
            TextOptions::default()
                .set_indexing_options(
                    TextFieldIndexing::default()
                        .set_tokenizer("path_prefix")  // ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼
                        .set_index_option(IndexRecordOption::Basic)
                )
                .set_stored()
        );
        
        let text_raw_field = schema_builder.add_text_field(
            "json_text_raw",
            TextOptions::default()
                .set_indexing_options(
                    TextFieldIndexing::default()
                        .set_tokenizer("raw")
                        .set_index_option(IndexRecordOption::Basic)
                )
                .set_stored()
        );
        
        // å°† number_field å’Œ date_field å®šä¹‰ä¸º text å­—æ®µï¼Œä½¿ç”¨ raw åˆ†è¯å™¨
        // ä»¥ä¾¿å­˜å‚¨ `path + encoded_value` å¹¶æ”¯æŒèŒƒå›´æŸ¥è¯¢
        let number_field = schema_builder.add_text_field(
            "json_number",
            TextOptions::default()
                .set_indexing_options(
                    TextFieldIndexing::default()
                        .set_tokenizer("raw")
                        .set_index_option(IndexRecordOption::Basic)
                )
        );
        
        let bool_field = schema_builder.add_bool_field(
            "json_bool",
            NumericOptions::default().set_indexed().set_stored().set_fast()
        );
        
        let date_field = schema_builder.add_text_field(
            "json_date",
            TextOptions::default()
                .set_indexing_options(
                    TextFieldIndexing::default()
                        .set_tokenizer("raw")
                        .set_index_option(IndexRecordOption::Basic)
                )
        );
        
        let schema = schema_builder.build();
        
        let path_tokenizer = PathPrefixTokenizer::new(config.path_separator.clone());
        
        Ok(FixedJsonLayer {
            text_analyzed_field,
            text_raw_field,
            number_field,
            bool_field,
            date_field,
            schema,
            config,
            path_tokenizer,
        })
    }
    
    /// åˆ›å»ºæˆ–æ‰“å¼€ç£ç›˜ç´¢å¼•
    pub fn create_or_open_index<P: AsRef<Path>>(&self, index_path: P) -> tantivy::Result<Index> {
        let index_path = index_path.as_ref();
        
        let index = if index_path.exists() {
            // æ‰“å¼€ç°æœ‰ç´¢å¼•
            println!("ğŸ“‚ Opening existing index at: {:?}", index_path);
            Index::open_in_dir(index_path)?
        } else {
            // åˆ›å»ºæ–°ç´¢å¼•
            println!("ğŸ†• Creating new index at: {:?}", index_path);
            std::fs::create_dir_all(index_path)?;
            Index::create_in_dir(index_path, self.schema.clone())?
        };
        
        // æ³¨å†Œè‡ªå®šä¹‰åˆ†è¯å™¨
        index.tokenizers()
            .register("path_prefix", PathPrefixTokenizer::new(self.config.path_separator.clone()));
        
        Ok(index)
    }
    
    pub fn schema(&self) -> &Schema {
        &self.schema
    }
    
    /// å¤„ç†æ‰å¹³ JSON å¯¹è±¡ï¼ˆä¸æ”¯æŒåµŒå¥—ï¼‰
    pub fn process_flat_json_object(&self, json_obj: &Map<String, Value>) -> tantivy::Result<TantivyDocument> {
        let mut doc = TantivyDocument::new();
        
        for (key, value) in json_obj {
            self.add_flat_value(&mut doc, key, value);
        }
        
        Ok(doc)
    }
    
    /// æ·»åŠ æ‰å¹³JSONå€¼ï¼ˆå¤„ç†æ•°ç»„å’ŒåŸºæœ¬ç±»å‹ï¼‰
    fn add_flat_value(&self, doc: &mut TantivyDocument, field_name: &str, value: &Value) {
        match value {
            Value::String(s) => {
                // å°è¯•è§£æä¸ºæ—¥æœŸï¼Œå¤±è´¥åˆ™ä½œä¸ºæ–‡æœ¬å¤„ç†
                if let Some(date_time) = self.try_parse_date(s) {
                    self.add_date_value(doc, field_name, date_time);
                } else {
                    let text_type = self.classify_text(s);
                    self.add_text_value(doc, field_name, s, text_type);
                }
            }
            Value::Number(n) => {
                if let Some(f) = n.as_f64() {
                    self.add_number_value(doc, field_name, f);
                }
            }
            Value::Bool(b) => {
                self.add_bool_value(doc, field_name, *b);
            }
            Value::Array(arr) => {
                // å¤„ç†æ•°ç»„ï¼šä¸ºæ¯ä¸ªå…ƒç´ æ·»åŠ ç›¸åŒçš„å­—æ®µå
                for item in arr {
                    self.add_flat_value(doc, field_name, item);
                }
            }
            _ => {
                // å¿½ç•¥ null å’Œå…¶ä»–ç±»å‹
            }
        }
    }
    
    /// å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
    fn try_parse_date(&self, s: &str) -> Option<DateTime> {
        // ç®€å•çš„æ—¥æœŸæ ¼å¼æ£€æŸ¥
        if s.len() < 8 {
            return None;
        }
        
        // æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼çš„åŸºæœ¬ç‰¹å¾
        let has_date_chars = s.contains('-') || s.contains('T') || s.contains(':');
        if !has_date_chars {
            return None;
        }
        
        // å°è¯•è§£æå¸¸è§æ—¥æœŸæ ¼å¼
        self.parse_date_formats(s)
    }
    
    /// è§£æå¤šç§æ—¥æœŸæ ¼å¼
    fn parse_date_formats(&self, s: &str) -> Option<DateTime> {
        use time::{PrimitiveDateTime, Date, Time};
        
        // 1. ISO 8601 with timezone: "2024-07-22T15:20:00Z"
        if let Ok(dt) = time::OffsetDateTime::parse(s, &time::format_description::well_known::Iso8601::DEFAULT) {
            return Some(DateTime::from_utc(dt));
        }
        
        // 2. ISO 8601 without timezone: "2024-07-22T15:20:00"
        if let Ok(dt) = PrimitiveDateTime::parse(s, &time::format_description::well_known::Iso8601::DEFAULT) {
            let offset_dt = dt.assume_utc();
            return Some(DateTime::from_utc(offset_dt));
        }
        
        // 3. Date only: "2024-07-22"
        if s.len() == 10 && s.matches('-').count() == 2 {
            if let Ok(date) = Date::parse(s, &time::format_description::parse("[year]-[month]-[day]").unwrap()) {
                let dt = PrimitiveDateTime::new(date, Time::MIDNIGHT);
                return Some(DateTime::from_utc(dt.assume_utc()));
            }
        }
        
        None
    }
    
    /// æ·»åŠ æ—¥æœŸå€¼
    fn add_date_value(&self, doc: &mut TantivyDocument, field_name: &str, date_time: DateTime) {
        let encoded_date = value_coder::encode_date(date_time);
        let path_value = format!("{}{}{}", field_name, self.config.path_separator, encoded_date);
        doc.add_text(self.date_field, &path_value);
    }
    
    /// ç®€åŒ–çš„æ–‡æœ¬åˆ†ç±»
    fn classify_text(&self, text: &str) -> TextType {
        // 1. æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡è¯†ç¬¦ï¼ˆé‚®ç®±ã€IDç­‰ç‰¹æ®Šæ ¼å¼ï¼‰
        for pattern in &self.config.text_classification_rules.identifier_patterns {
            if pattern.is_match(text) {
                return TextType::Identifier;
            }
        }
        
        // 2. æ£€æŸ¥æ˜¯å¦åŒ…å«ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·
        if self.has_whitespace_or_punctuation(text) {
            TextType::AnalyzedText  // éœ€è¦åˆ†è¯
        } else {
            TextType::Keyword       // ç®€çŸ­å…³é”®è¯
        }
    }
    
    /// æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·
    fn has_whitespace_or_punctuation(&self, text: &str) -> bool {
        text.chars().any(|c| {
            c.is_whitespace() || 
            c.is_ascii_punctuation() ||
            !c.is_alphanumeric()
        })
    }
    
    /// æ·»åŠ æ–‡æœ¬å€¼ - æ™ºèƒ½åˆ†è¯ç­–ç•¥
    fn add_text_value(&self, doc: &mut TantivyDocument, path: &str, value: &str, text_type: TextType) {
        let prefixed_value = format!("{}{}{}", path, self.config.path_separator, value);
        
        match text_type {
            TextType::AnalyzedText => {
                // 1. åŸå§‹å­—æ®µï¼šå®Œæ•´æ–‡æœ¬+è·¯å¾„å‰ç¼€ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ï¼‰
                doc.add_text(self.text_raw_field, &prefixed_value);
                
                // 2. åˆ†æå­—æ®µï¼šä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œæ¯ä¸ªtokenå¸¦è·¯å¾„å‰ç¼€
                let tokens = self.path_tokenizer.clone().tokenize_to_strings(&prefixed_value);
                for token in tokens {
                    doc.add_text(self.text_analyzed_field, &token);
                }
            }
            TextType::Keyword | TextType::Identifier => {
                // å…³é”®è¯å’Œæ ‡è¯†ç¬¦åªæ·»åŠ åˆ°åŸå§‹å­—æ®µï¼ˆrawåˆ†è¯å™¨ï¼‰
                doc.add_text(self.text_raw_field, &prefixed_value);
            }
        }
    }
    
    /// æ·»åŠ æ•°å€¼
    fn add_number_value(&self, doc: &mut TantivyDocument, path: &str, value: f64) {
        let encoded_num = value_coder::encode_f64(value);
        let path_value = format!("{}{}{}", path, self.config.path_separator, encoded_num);
        doc.add_text(self.number_field, &path_value);
    }
    
    /// æ·»åŠ å¸ƒå°”å€¼
    fn add_bool_value(&self, doc: &mut TantivyDocument, path: &str, value: bool) {
        doc.add_bool(self.bool_field, value);
        
        let path_value = format!("{}{}{}_{}", path, self.config.path_separator, "bool", value);
        doc.add_text(self.text_raw_field, &path_value);
    }
}

/// æ–‡æœ¬ç±»å‹åˆ†ç±»
#[derive(Debug, Clone)]
enum TextType {
    AnalyzedText,  // éœ€è¦åˆ†è¯çš„æ–‡æœ¬
    Keyword,       // çŸ­å…³é”®è¯
    Identifier,    // æ ‡è¯†ç¬¦
}

/// æ™ºèƒ½æŸ¥è¯¢æ„å»ºå™¨
pub struct SmartJsonQueryBuilder {
    layer: FixedJsonLayer,
}

impl SmartJsonQueryBuilder {
    pub fn new(layer: FixedJsonLayer) -> Self {
        Self { layer }
    }
    
    /// æ™ºèƒ½è·¯å¾„æŸ¥è¯¢ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æŸ¥è¯¢ç­–ç•¥
    pub fn smart_query(&self, path: &str, value: &str) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use tantivy::query::{TermQuery, BooleanQuery, Occur};
        
        let prefixed_value = format!("{}{}{}", path, self.layer.config.path_separator, value);
        
        // åˆ›å»ºå¤šä¸ªæŸ¥è¯¢é€‰é¡¹
        let mut subqueries = Vec::new();
        
        // 1. åŸå§‹å­—æ®µæŸ¥è¯¢ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        let raw_term = Term::from_field_text(self.layer.text_raw_field, &prefixed_value);
        subqueries.push((Occur::Should, Box::new(TermQuery::new(raw_term, IndexRecordOption::Basic)) as Box<dyn tantivy::query::Query>));
        
        // 2. åˆ†è¯å­—æ®µæŸ¥è¯¢ï¼ˆå¦‚æœå¯èƒ½åŒ…å«å¯åˆ†ææ–‡æœ¬ï¼‰
        if value.len() >= 3 {  // å¯¹äºè¾ƒé•¿çš„å€¼ä¹Ÿå°è¯•åˆ†è¯æŸ¥è¯¢
            let analyzed_term = Term::from_field_text(self.layer.text_analyzed_field, &prefixed_value);
            subqueries.push((Occur::Should, Box::new(TermQuery::new(analyzed_term, IndexRecordOption::Basic)) as Box<dyn tantivy::query::Query>));
        }
        
        if subqueries.len() == 1 {
            Ok(subqueries.into_iter().next().unwrap().1)
        } else {
            Ok(Box::new(BooleanQuery::new(subqueries)))
        }
    }
    
    /// ç²¾ç¡®åŒ¹é…æŸ¥è¯¢
    pub fn exact_query(&self, path: &str, value: &str) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use tantivy::query::TermQuery;
        
        let prefixed_value = format!("{}{}{}", path, self.layer.config.path_separator, value);
        let term = Term::from_field_text(self.layer.text_raw_field, &prefixed_value);
        Ok(Box::new(TermQuery::new(term, IndexRecordOption::Basic)))
    }
    
    // /// æ•°å€¼èŒƒå›´æŸ¥è¯¢ï¼ˆå…¨å±€ï¼‰
    // pub fn number_range_query(&self, min: f64, max: f64) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
    //     use tantivy::query::RangeQuery;
    //     use std::ops::Bound;
        
    //     let min_term = Term::from_field_f64(self.layer.number_field, min);
    //     let max_term = Term::from_field_f64(self.layer.number_field, max);
        
    //     Ok(Box::new(RangeQuery::new(
    //         Bound::Included(min_term),
    //         Bound::Included(max_term)
    //     )))
    // }
    
    /// å¸¦è·¯å¾„çš„æ•°å€¼èŒƒå›´æŸ¥è¯¢ - ä¸¤é˜¶æ®µç­–ç•¥
    pub fn number_range_query_with_path(&self, path: &str, min: f64, max: f64) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use tantivy::query::RangeQuery;
        use std::ops::Bound;
        
        let path_prefix = format!("{}{}", path, self.layer.config.path_separator);

        let min_str = format!("{}{}", path_prefix, value_coder::encode_f64(min));
        let max_str = format!("{}{}", path_prefix, value_coder::encode_f64(max));

        let min_term = Term::from_field_text(self.layer.number_field, &min_str);
        let max_term = Term::from_field_text(self.layer.number_field, &max_str);

        let range_query = RangeQuery::new(
            Bound::Included(min_term),
            Bound::Included(max_term)
        );
        
        Ok(Box::new(range_query))
    }
    
    // /// æ—¥æœŸèŒƒå›´æŸ¥è¯¢ï¼ˆå…¨å±€ï¼‰
    // pub fn date_range_query(&self, start_date: &str, end_date: &str) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
    //     use tantivy::query::RangeQuery;
    //     use std::ops::Bound;
        
    //     let start_dt = self.layer.parse_date_formats(start_date)
    //         .ok_or_else(|| tantivy::TantivyError::InvalidArgument(format!("Cannot parse start date: {}", start_date)))?;
    //     let end_dt = self.layer.parse_date_formats(end_date)
    //         .ok_or_else(|| tantivy::TantivyError::InvalidArgument(format!("Cannot parse end date: {}", end_date)))?;
            
    //     let start_term = Term::from_field_date(self.layer.date_field, start_dt);
    //     let end_term = Term::from_field_date(self.layer.date_field, end_dt);
        
    //     Ok(Box::new(RangeQuery::new(
    //         Bound::Included(start_term),
    //         Bound::Included(end_term)
    //     )))
    // }
    
    /// å¸¦è·¯å¾„çš„æ—¥æœŸèŒƒå›´æŸ¥è¯¢ - ä¸¤é˜¶æ®µç­–ç•¥
    pub fn date_range_query_with_path(&self, path: &str, start_date: &str, end_date: &str) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use tantivy::query::RangeQuery;
        use std::ops::Bound;
        
        let start_dt = self.layer.parse_date_formats(start_date)
            .ok_or_else(|| tantivy::TantivyError::InvalidArgument(format!("Cannot parse start date: {}", start_date)))?;
        let end_dt = self.layer.parse_date_formats(end_date)
            .ok_or_else(|| tantivy::TantivyError::InvalidArgument(format!("Cannot parse end date: {}", end_date)))?;
            
        let path_prefix = format!("{}{}", path, self.layer.config.path_separator);

        let start_str = format!("{}{}", path_prefix, value_coder::encode_date(start_dt));
        let end_str = format!("{}{}", path_prefix, value_coder::encode_date(end_dt));

        let start_term = Term::from_field_text(self.layer.date_field, &start_str);
        let end_term = Term::from_field_text(self.layer.date_field, &end_str);
        
        let range_query = RangeQuery::new(
            Bound::Included(start_term),
            Bound::Included(end_term)
        );
        
        Ok(Box::new(range_query))
    }
    
    /// æ—¥æœŸç²¾ç¡®æŸ¥è¯¢
    pub fn date_exact_query(&self, path: &str, date_str: &str) -> tantivy::Result<Box<dyn tantivy::query::Query>> {
        use tantivy::query::TermQuery;
        
        let date_time = self.layer.parse_date_formats(date_str)
            .ok_or_else(|| tantivy::TantivyError::InvalidArgument(format!("Cannot parse date: {}", date_str)))?;
            
        let encoded_date = value_coder::encode_date(date_time);
        let path_value = format!("{}{}{}", path, self.layer.config.path_separator, encoded_date);
        
        let term = Term::from_field_text(self.layer.date_field, &path_value);
        Ok(Box::new(TermQuery::new(term, IndexRecordOption::Basic)))
    }
}

fn main() -> tantivy::Result<()> {
    use serde_json::json;
    use tantivy::collector::TopDocs;
    
    println!("ğŸš€ Optimized JSON Processing Layer - Flat Structure + Disk Persistence");
    println!("ğŸ“‹ Features: Flat JSON, custom tokenizer, disk persistence, array support");
    
    // åˆ›å»ºä¿®å¤ç‰ˆ JSON å¤„ç†å±‚
    let layer = FixedJsonLayer::new()?;
    
    // æµ‹è¯•è‡ªå®šä¹‰åˆ†è¯å™¨å’Œæ–‡æœ¬åˆ†ç±»
    println!("\n=== ğŸ”¬ Testing Custom Tokenizer & Text Classification ===");
    let test_text = "description__A high-quality search engine library for Rust applications";
    let tokens = layer.path_tokenizer.clone().tokenize_to_strings(test_text);
    println!("ğŸ“ Input: {}", test_text);
    println!("ğŸ”— Tokens: {:?}", tokens);
    println!("âœ¨ Each token preserves path prefix for precise targeting!");
    
    // æµ‹è¯•ç®€åŒ–çš„æ–‡æœ¬åˆ†ç±»è§„åˆ™
    println!("\n=== ğŸ“Š Text Classification Examples ===");
    let test_cases = vec![
        ("rust", "çŸ­å…³é”®è¯ï¼Œæ— ç©ºæ ¼"),
        ("search engine", "åŒ…å«ç©ºæ ¼ï¼Œéœ€è¦åˆ†è¯"),
        ("alice@example.com", "é‚®ç®±æ ¼å¼ï¼Œæ ‡è¯†ç¬¦"),
        ("PROD123456", "äº§å“IDï¼Œæ ‡è¯†ç¬¦"),
        ("WH001234", "SKUæ ¼å¼ï¼Œæ ‡è¯†ç¬¦"),
        ("high-quality", "åŒ…å«è¿å­—ç¬¦ï¼Œéœ€è¦åˆ†è¯"),
        ("San Francisco", "åŒ…å«ç©ºæ ¼ï¼Œéœ€è¦åˆ†è¯"),
        ("python", "çŸ­å…³é”®è¯ï¼Œæ— ç©ºæ ¼"),
    ];
    
    for (text, description) in test_cases {
        let text_type = layer.classify_text(text);
        let type_str = match text_type {
            TextType::AnalyzedText => "AnalyzedText",
            TextType::Keyword => "Keyword",
            TextType::Identifier => "Identifier",
        };
        println!("   '{}' â†’ {} ({})", text, type_str, description);
    }
    
    // åˆ›å»ºç£ç›˜ç´¢å¼•
    let index_path = "./json_index";
    let index = layer.create_or_open_index(index_path)?;
    
    let mut index_writer = index.writer(50_000_000)?;
    
    // æ‰å¹³JSONæµ‹è¯•æ•°æ®é›†
    let test_documents = vec![
        // æ–‡æ¡£1ï¼šç”¨æˆ·æ¡£æ¡ˆï¼ˆæ‰å¹³ç»“æ„ï¼‰
        json!({
            "user_name": "Alice Smith",
            "user_email": "alice@example.com",
            "user_age": 28, 
            "user_tags": ["rust", "search", "database"],
            "user_scores": [95, 87, 92],
            "user_languages": ["english", "chinese"],
            "user_created_at": "2024-01-15T10:30:00Z",
            "user_last_login": "2024-07-20T14:25:00Z",
            "product_id": "PROD123456",
            "product_price": 99.99,
            "product_active": true,
            "product_description": "A high-quality search engine library for Rust applications",
            "product_release_date": "2024-03-10"
        }),
        
        // æ–‡æ¡£2ï¼šä¼ä¸šä¿¡æ¯ï¼ˆæ‰å¹³ç»“æ„ï¼‰
        json!({
            "company_name": "Tech Innovations Inc",
            "company_email": "contact@techinnovations.com",
            "company_founded": 2020,
            "company_categories": ["software", "AI", "machine learning"],
            "company_ratings": [4.8, 4.5, 4.9],
            "company_technologies": ["python", "rust", "tensorflow"],
            "company_country": "USA",
            "company_city": "San Francisco",
            "company_coordinates": [37.7749, -122.4194],
            "company_established_date": "2020-01-15T09:00:00Z",
            "company_last_updated": "2024-07-22T16:45:00Z",
            "user_age": 25

        }),
        
        // æ–‡æ¡£3ï¼šç”µå•†äº§å“ï¼ˆæ‰å¹³ç»“æ„ï¼‰
        json!({
            "product_name": "Wireless Headphones",
            "product_sku": "WH001234",
            "product_price": 149.99,
            "review_ratings": [5, 4, 5, 3, 4],
            "review_comments": ["excellent quality", "great sound", "comfortable"],
            "review_verified": [true, true, false, true, true],
            "inventory_stock": 50,
            "inventory_colors": ["black", "white", "blue"],
            "inventory_availability": true,
            "product_launch_date": "2024-02-14",
            "test_wrong":25,
            "user_age": 80,
            "inventory_last_updated": "2024-07-21T08:30:00Z"
        }),
        
        // æ–‡æ¡£4ï¼šå­¦æœ¯è®ºæ–‡ï¼ˆæ‰å¹³ç»“æ„ï¼‰
        json!({
            "paper_title": "Advanced Information Retrieval Systems",
            "paper_authors": ["Dr. John Smith", "Prof. Jane Doe"],
            "paper_year": 2023,
            "test_wrong":149.99,
            "product_price": 19.99,
            "metrics_citations": 42,
            "metrics_downloads": [120, 95, 87, 76],
            "metrics_impact_factor": 2.8,
            "research_keywords": ["information retrieval", "search engines", "natural language processing"],
            "paper_published_date": "2023-05-20T12:00:00Z",
            "paper_submitted_date": "2023-02-28",
            "metrics_last_calculated": "2024-07-15T10:00:00Z"
        })
    ];
    
    // ç´¢å¼•æ‰€æœ‰æ–‡æ¡£
    for (i, json_data) in test_documents.iter().enumerate() {
        if let Value::Object(obj) = json_data {
            let doc = layer.process_flat_json_object(obj)?;
            index_writer.add_document(doc)?;
            println!("âœ… Document {} indexed successfully", i + 1);
        }
    }
    
    index_writer.commit()?;
    
    let query_builder = SmartJsonQueryBuilder::new(layer);
    let reader = index.reader()?;
    let searcher = reader.searcher();
    
    // ç»¼åˆæŸ¥è¯¢æµ‹è¯•
    println!("\n=== ğŸ” Comprehensive Query Tests ===");
    
    // åŸºç¡€æ–‡æœ¬æŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ“ === Text Query Tests ===");
    
    // 1. æ•°ç»„ä¸­çš„exactæŸ¥è¯¢
    println!("\n1. Array exact query for 'rust' in user_tags:");
    let query = query_builder.smart_query("user_tags", "rust")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 2. æ•°ç»„ä¸­çš„exactæŸ¥è¯¢ - æŠ€æœ¯æ ˆ
    println!("\n2. Array exact query for 'python' in company_technologies:");
    let query = query_builder.smart_query("company_technologies", "python")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 3. é•¿æ–‡æœ¬åˆ†è¯æŸ¥è¯¢
    println!("\n3. Long text token query for 'library' in product_description:");
    let query = query_builder.smart_query("product_description", "library")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 4. å­¦æœ¯å…³é”®è¯æŸ¥è¯¢
    println!("\n4. Academic keyword query for 'information' in research_keywords:");
    let query = query_builder.smart_query("research_keywords", "information")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // ç²¾ç¡®åŒ¹é…æŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ¯ === Exact Match Tests ===");
    
    // 5. äº§å“SKUç²¾ç¡®æŸ¥è¯¢
    println!("\n5. Exact query for product_sku:");
    let query = query_builder.exact_query("product_sku", "WH001234")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 6. é‚®ç®±ç²¾ç¡®æŸ¥è¯¢
    println!("\n6. Exact query for company_email:");
    let query = query_builder.exact_query("company_email", "contact@techinnovations.com")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // æ•°å€¼èŒƒå›´æŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ”¢ === Number Range Query Tests ===");
    
    // 7. å¸¦è·¯å¾„çš„ä»·æ ¼èŒƒå›´æŸ¥è¯¢
    println!("\n7. Price range query with path (140-160) for product_price:");
    let query = query_builder.number_range_query_with_path("product_price", 140.0, 160.0)?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 8. å¸¦è·¯å¾„çš„å¹´é¾„èŒƒå›´æŸ¥è¯¢
    println!("\n8. Age range query with path (25-30) for user_age:");
    let query = query_builder.number_range_query_with_path("user_age", 25.0, 30.0)?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 9. å¸¦è·¯å¾„çš„è¯„åˆ†èŒƒå›´æŸ¥è¯¢
    println!("\n9. Rating range query with path (4.0-5.0) for company_ratings:");
    let query = query_builder.number_range_query_with_path("company_ratings", 4.0, 5.0)?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // æ•°ç»„å†…å®¹æŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ“š === Array Content Query Tests ===");
    
    // 10. é¢œè‰²æ•°ç»„æŸ¥è¯¢
    println!("\n10. Array exact query for 'black' in inventory_colors:");
    let query = query_builder.smart_query("inventory_colors", "black")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 11. è¯„è®ºæ•°ç»„æŸ¥è¯¢
    println!("\n11. Array text query for 'excellent' in review_comments:");
    let query = query_builder.smart_query("review_comments", "excellent")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 12. è¯­è¨€åå¥½æŸ¥è¯¢
    println!("\n12. Array exact query for 'chinese' in user_languages:");
    let query = query_builder.smart_query("user_languages", "chinese")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // å¤æ‚æŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ”„ === Complex Query Tests ===");
    
    // 13. åŸå¸‚åœ°ç†æŸ¥è¯¢
    println!("\n13. Geographic query for 'San Francisco' in company_city:");
    let query = query_builder.smart_query("company_city", "San Francisco")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 14. ä½œè€…æŸ¥è¯¢ - ä½¿ç”¨ç²¾ç¡®åŒ¹é…
    println!("\n14. Author exact query for 'Dr. John Smith' in paper_authors:");
    let query = query_builder.exact_query("paper_authors", "Dr. John Smith")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // æ—¥æœŸæŸ¥è¯¢æµ‹è¯•
    println!("\nğŸ“… === Date Query Tests ===");
    
    // 15. å¸¦è·¯å¾„çš„ç”¨æˆ·åˆ›å»ºæ—¶é—´èŒƒå›´æŸ¥è¯¢
    println!("\n15. Date range query with path for user_created_at (Jan 2024):");
    let query = query_builder.date_range_query_with_path("user_created_at", "2024-01-01T00:00:00Z", "2024-01-31T23:59:59Z")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 16. å¸¦è·¯å¾„çš„äº§å“å‘å¸ƒæ—¥æœŸç²¾ç¡®æŸ¥è¯¢
    println!("\n16. Date exact query with path for product_launch_date:");
    let query = query_builder.date_exact_query("product_launch_date", "2024-02-14")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    // 17. å¸¦è·¯å¾„çš„æœ€è¿‘æ›´æ–°æŸ¥è¯¢
    println!("\n17. Recent updates query with path for company_last_updated (July 2024):");
    let query = query_builder.date_range_query_with_path("company_last_updated", "2024-07-01T00:00:00Z", "2024-07-31T23:59:59Z")?;
    let results = searcher.search(&*query, &TopDocs::with_limit(10))?;
    println!("   Results: {} documents found {}", results.len(), if results.len() > 0 { "âœ…" } else { "âŒ" });
    
    
    
    println!("\nğŸ’¡ Index Location: './json_index' (will persist between runs)");
    
    Ok(())
} 